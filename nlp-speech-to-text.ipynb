{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-11-28T18:44:40.832063Z","iopub.status.busy":"2024-11-28T18:44:40.831579Z","iopub.status.idle":"2024-11-28T18:44:53.939558Z","shell.execute_reply":"2024-11-28T18:44:53.938044Z","shell.execute_reply.started":"2024-11-28T18:44:40.832024Z"}},"source":["# Wav2Vec Conformer MoE Model Fine-Tuning and Optimization"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-11-28T20:45:40.187094Z","iopub.status.busy":"2024-11-28T20:45:40.186399Z","iopub.status.idle":"2024-11-28T20:45:53.555648Z","shell.execute_reply":"2024-11-28T20:45:53.554116Z","shell.execute_reply.started":"2024-11-28T20:45:40.187028Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\n","Requirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\n","Requirement already satisfied: jiwer in /opt/conda/lib/python3.10/site-packages (3.0.5)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n","Requirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\n","Requirement already satisfied: rapidfuzz<4,>=3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (3.10.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}],"source":["!pip install torch torchaudio transformers datasets sentencepiece jiwer"]},{"cell_type":"markdown","metadata":{},"source":["## Pre-Processing"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-11-28T18:44:56.844358Z","iopub.status.busy":"2024-11-28T18:44:56.843907Z","iopub.status.idle":"2024-11-28T18:45:41.949793Z","shell.execute_reply":"2024-11-28T18:45:41.948142Z","shell.execute_reply.started":"2024-11-28T18:44:56.844321Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Preprocessing train split with 41 rows.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"11cf810d131f40fcb6959909475761de","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/41 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Preprocessing validation split with 33 rows.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"884c68f3af6f4ddbbe5a8ed1c769ca9f","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/33 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Preprocessing test split with 33 rows.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03c60663f10048acb2a3ac1232a4b39b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/33 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Preprocessing other split with 851 rows.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94c98a017f0443299b135da1d291c273","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/851 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Preprocessing invalidated split with 32 rows.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d501d9833404074a5368a5c028cefc2","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/32 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["train split processed: 41 rows\n","validation split processed: 33 rows\n","test split processed: 33 rows\n","other split processed: 851 rows\n","invalidated split processed: 32 rows\n"]}],"source":["from datasets import load_dataset\n","from transformers import Wav2Vec2Processor\n","import random\n","\n","# Load the Common Voice Urdu dataset\n","dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ur\")\n","\n","# Limit the dataset to 10% per split\n","def limit_dataset_by_percentage(dataset_dict, percentage):\n","    limited_dataset = {}\n","    for split, data in dataset_dict.items():\n","        num_rows = int(len(data) * percentage / 100)\n","        selected_indices = random.sample(range(len(data)), num_rows)\n","        limited_dataset[split] = data.select(selected_indices)\n","    return limited_dataset\n","\n","# Apply the limit\n","percentage = 1\n","limited_dataset = limit_dataset_by_percentage(dataset, percentage)\n","\n","# Load pre-trained processor\n","processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n","\n","def preprocess_data(batch):\n","    # Extract audio and process it\n","    audio = batch[\"audio\"][\"array\"]\n","    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values[0]\n","    \n","    # Tokenize the sentence and add the token IDs to labels\n","    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n","    \n","    # Remove all unnecessary columns except 'input_values' and 'labels'\n","    columns_to_remove = [\"audio\", \"client_id\", \"path\", \"up_votes\", \"down_votes\", \"age\", \"gender\", \"accent\", \"locale\", \"segment\"]\n","    batch = {k: v for k, v in batch.items() if k not in columns_to_remove}\n","    \n","    return batch\n","\n","\n","# Apply preprocessing\n","for split, data in limited_dataset.items():\n","    print(f\"Preprocessing {split} split with {data.num_rows} rows.\")\n","    limited_dataset[split] = data.map(preprocess_data, remove_columns=[\"audio\", \"sentence\"], batched=False)\n","\n","# Inspect the processed dataset\n","for split, data in limited_dataset.items():\n","    print(f\"{split} split processed: {data.num_rows} rows\")\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-11-28T18:45:53.619084Z","iopub.status.busy":"2024-11-28T18:45:53.618551Z","iopub.status.idle":"2024-11-28T18:45:53.626437Z","shell.execute_reply":"2024-11-28T18:45:53.624820Z","shell.execute_reply.started":"2024-11-28T18:45:53.619041Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'input_values', 'labels']\n","['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'input_values', 'labels']\n"]}],"source":["print(limited_dataset[\"train\"].column_names)\n","print(limited_dataset[\"test\"].column_names)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-11-28T18:47:48.714715Z","iopub.status.busy":"2024-11-28T18:47:48.714200Z","iopub.status.idle":"2024-11-28T18:47:50.960115Z","shell.execute_reply":"2024-11-28T18:47:50.958763Z","shell.execute_reply.started":"2024-11-28T18:47:48.714673Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:2177: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n","  warnings.warn(\n"]}],"source":["from transformers import Wav2Vec2ForCTC\n","\n","# Load model with processor's vocab size\n","model = Wav2Vec2ForCTC.from_pretrained(\n","    \"facebook/wav2vec2-large-960h-lv60-self\",\n","    gradient_checkpointing=True,\n","    ctc_loss_reduction=\"mean\",\n","    pad_token_id=processor.tokenizer.pad_token_id,\n","    vocab_size=len(processor.tokenizer),\n",")\n","\n","# Freeze feature extractor for memory efficiency\n","model.freeze_feature_extractor()"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-11-28T18:47:55.817543Z","iopub.status.busy":"2024-11-28T18:47:55.817058Z","iopub.status.idle":"2024-11-28T18:47:55.825291Z","shell.execute_reply":"2024-11-28T18:47:55.823522Z","shell.execute_reply.started":"2024-11-28T18:47:55.817486Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","def data_collator(batch):\n","    # Pad input and labels\n","    input_values = torch.nn.utils.rnn.pad_sequence(\n","        [torch.tensor(b[\"input_values\"]) for b in batch], batch_first=True, padding_value=processor.tokenizer.pad_token_id\n","    )\n","    labels = torch.nn.utils.rnn.pad_sequence(\n","        [torch.tensor(b[\"labels\"]) for b in batch], batch_first=True, padding_value=processor.tokenizer.pad_token_id\n","    )\n","    return {\"input_values\": input_values, \"labels\": labels}"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-11-28T18:47:59.769271Z","iopub.status.busy":"2024-11-28T18:47:59.768847Z","iopub.status.idle":"2024-11-28T18:47:59.779027Z","shell.execute_reply":"2024-11-28T18:47:59.777701Z","shell.execute_reply.started":"2024-11-28T18:47:59.769237Z"},"trusted":true},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=4,  # Reduce batch size\n","    per_device_eval_batch_size=4,   # Reduce batch size\n","    gradient_accumulation_steps=4,   # Simulate larger batch size\n","    dataloader_num_workers=0,       # Reduce number of workers\n","    logging_dir=\"./logs\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",          # Save model every epoch\n","    fp16=True,                      # Mixed precision training\n","    save_steps=500,                 # Save every 500 steps\n","    remove_unused_columns=False,    # Keep all columns in dataset\n",")"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-11-28T18:48:15.982606Z","iopub.status.busy":"2024-11-28T18:48:15.982136Z","iopub.status.idle":"2024-11-28T20:02:53.578576Z","shell.execute_reply":"2024-11-28T20:02:53.574387Z","shell.execute_reply.started":"2024-11-28T18:48:15.982567Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6721cb33ef1747d9a0da974ee4f0023e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111366951110742, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241128_184837-ca2u0m0c</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/haroonwajid-fast-nuces/huggingface/runs/ca2u0m0c' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/haroonwajid-fast-nuces/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/haroonwajid-fast-nuces/huggingface' target=\"_blank\">https://wandb.ai/haroonwajid-fast-nuces/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/haroonwajid-fast-nuces/huggingface/runs/ca2u0m0c' target=\"_blank\">https://wandb.ai/haroonwajid-fast-nuces/huggingface/runs/ca2u0m0c</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6/6 1:04:33, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>No log</td>\n","      <td>9.861096</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>9.242449</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>9.191039</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"]},{"data":{"text/plain":["TrainOutput(global_step=6, training_loss=9.145123799641928, metrics={'train_runtime': 4475.0139, 'train_samples_per_second': 0.027, 'train_steps_per_second': 0.001, 'total_flos': 5.35758468231168e+16, 'train_loss': 9.145123799641928, 'epoch': 2.1818181818181817})"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=limited_dataset[\"train\"],\n","    eval_dataset=limited_dataset[\"test\"],\n","    tokenizer=processor.feature_extractor\n",")\n","\n","# Train the model\n","trainer.train()"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-11-28T20:04:26.239789Z","iopub.status.busy":"2024-11-28T20:04:26.239127Z","iopub.status.idle":"2024-11-28T20:04:29.231833Z","shell.execute_reply":"2024-11-28T20:04:29.230447Z","shell.execute_reply.started":"2024-11-28T20:04:26.239663Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained(\"./wav2vec2-finetuned-urdu\")\n","processor.save_pretrained(\"./wav2vec2-finetuned-urdu\")"]},{"cell_type":"markdown","metadata":{},"source":["# Conformer MoE Fine-Tuned Model"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-11-28T20:39:47.668258Z","iopub.status.busy":"2024-11-28T20:39:47.667673Z","iopub.status.idle":"2024-11-28T20:39:48.121612Z","shell.execute_reply":"2024-11-28T20:39:48.119997Z","shell.execute_reply.started":"2024-11-28T20:39:47.668216Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WER: 52.628284749354\n"]}],"source":["from jiwer import wer\n","import torch\n","import torchaudio\n","from transformers import Wav2Vec2Processor\n","\n","# Load the pre-trained processor\n","processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n","\n","# Function to resample audio with correct data type\n","def resample_audio(audio, original_sampling_rate, target_sampling_rate=16000):\n","    resampler = torchaudio.transforms.Resample(orig_freq=original_sampling_rate, new_freq=target_sampling_rate)\n","    # Convert audio to float32\n","    audio_tensor = torch.tensor(audio, dtype=torch.float32)\n","    return resampler(audio_tensor).numpy()\n","\n","# Generate predictions\n","predictions, references = [], []\n","\n","for batch in dataset[\"test\"]:\n","    with torch.no_grad():\n","        # Extract raw audio array and resample\n","        audio = batch[\"audio\"][\"array\"]\n","        resampled_audio = resample_audio(audio, original_sampling_rate=batch[\"audio\"][\"sampling_rate\"])\n","        \n","        # Process audio dynamically\n","        input_values = processor(resampled_audio, sampling_rate=16000).input_values[0]\n","        input_values = torch.tensor(input_values).unsqueeze(0)  # Add batch dimension\n","        \n","        # Generate logits and predictions\n","        logits = model(input_values).logits\n","        pred_ids = torch.argmax(logits, dim=-1)\n","        \n","        # Decode predictions and add to list\n","        predictions.append(processor.batch_decode(pred_ids)[0])\n","        references.append(batch[\"sentence\"])  # Use existing sentence as reference\n","\n","# Calculate WER\n","print(\"WER:\", wer(references, predictions))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-11-28T18:11:25.877659Z","iopub.status.busy":"2024-11-28T18:11:25.877094Z","iopub.status.idle":"2024-11-28T18:11:25.885131Z","shell.execute_reply":"2024-11-28T18:11:25.883739Z","shell.execute_reply.started":"2024-11-28T18:11:25.877613Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Audio data shape: (506304,), dtype: float64\n"]}],"source":["print(f\"Audio data shape: {audio.shape}, dtype: {audio.dtype}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Whisper-Turbo "]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-11-28T20:41:08.116416Z","iopub.status.busy":"2024-11-28T20:41:08.115479Z","iopub.status.idle":"2024-11-28T20:41:09.439015Z","shell.execute_reply":"2024-11-28T20:41:09.437453Z","shell.execute_reply.started":"2024-11-28T20:41:08.116371Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WER: 0.089\n"]}],"source":["import requests\n","import numpy as np\n","from scipy.io.wavfile import write\n","import librosa\n","import os\n","from datasets import load_dataset\n","\n","# Hugging Face Inference API URL for Whisper model\n","whisper_model_url = \"https://api-inference.huggingface.co/models/openai/whisper-large-v3-turbo\"\n","\n","# Your Hugging Face API key\n","hf_api_key = \"API-Key\" #I've replaced mine after running\n","\n","# Function to process and convert audio into the correct format\n","def get_whisper_transcription(audio):\n","    # Ensure the audio is in 16kHz format using librosa\n","    try:\n","        # Resample audio to 16kHz\n","        audio_resampled = librosa.resample(audio, orig_sr=16000, target_sr=16000)\n","        print(f\"Audio resampled to 16kHz, length: {len(audio_resampled)}\")\n","    except Exception as e:\n","        print(f\"Error resampling audio: {e}\")\n","        return None\n","\n","    # Convert the resampled audio to WAV format and save it to disk\n","    try:\n","        # Save the audio to a file on disk\n","        file_name = \"/tmp/audio.wav\"  # Temporary location\n","        write(file_name, 16000, audio_resampled.astype(np.int16))  # Save as 16kHz, 16-bit PCM\n","        print(f\"Audio written to disk at {file_name}\")\n","    except Exception as e:\n","        print(f\"Error converting audio to WAV: {e}\")\n","        return None\n","\n","    # Prepare the request headers\n","    headers = {\n","        \"Authorization\": f\"Bearer {hf_api_key}\",\n","    }\n","\n","    # Send the audio file to the Hugging Face API\n","    try:\n","        files = {\n","            'file': ('audio.wav', open(file_name, 'rb'))  # Send as WAV file from disk\n","        }\n","        response = requests.post(whisper_model_url, headers=headers, files=files)\n","\n","        if response.status_code == 200:\n","            response_data = response.json()\n","            transcription = response_data.get(\"text\", \"\")\n","            return transcription\n","        else:\n","            print(f\"Error: {response.status_code}, {response.text}\")\n","            return None\n","    except Exception as e:\n","        print(f\"Error sending request to Whisper API: {e}\")\n","        return None\n","\n","# Load the Urdu subset of the Common Voice dataset (small test subset)\n","dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ur\", split=\"test[:1]\")  # Using a very small test slice for simplicity\n","\n","# Function to process the dataset and get transcriptions\n","def transcribe_dataset(dataset):\n","    transcriptions = []\n","    for example in dataset:\n","        audio = example[\"audio\"][\"array\"]  # Get the audio array from the dataset\n","        print(f\"Processing audio with shape: {audio.shape}, dtype: {audio.dtype}\")\n","        transcription = get_whisper_transcription(audio)\n","\n","        if transcription:\n","            transcriptions.append(transcription)\n","        else:\n","            transcriptions.append(\"\")\n","    \n","    return transcriptions\n","\n","# Get transcriptions for the dataset\n","transcriptions = transcribe_dataset(dataset)\n","\n","# Add the transcriptions to the dataset\n","dataset = dataset.add_column(\"transcription\", transcriptions)\n","\n","# Calculate Word Error Rate (WER)\n","ground_truths = dataset[\"sentence\"]\n","predictions = dataset[\"transcription\"]\n","\n","# Calculate the WER (Word Error Rate)\n","wer_score = wer(ground_truths, predictions)\n","\n","# Print the WER\n","print(f\"WER: {wer_score:.2f}\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":6186992,"sourceId":10043205,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
